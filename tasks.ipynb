{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25cd1d9a-4b66-4ac0-9203-bfc1d3f41a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d6993-240d-4d8a-88ca-eadeb19bad6a",
   "metadata": {},
   "source": [
    "# ***TASK 1***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375fddab-bcac-4b33-8820-9066af26446a",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0765578-f155-43c8-a1d3-a87adab4974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4070, 9)\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('./data/Task1_2.csv', sep=';')\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89bde1ac-9b2f-4b3b-8040-0e39fede4fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POUG</th>\n",
       "      <th>TRE</th>\n",
       "      <th>ID</th>\n",
       "      <th>ZUB</th>\n",
       "      <th>VOL</th>\n",
       "      <th>UIO</th>\n",
       "      <th>VBNM</th>\n",
       "      <th>Type</th>\n",
       "      <th>OIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.750</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>uuuu</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>17.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.290</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>wwww</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>16.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>wwww</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>31.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.335</td>\n",
       "      <td>3</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>uuuu</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>48.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>4</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>wwww</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>32.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   POUG    TRE  ID ZUB VOL   UIO VBNM Type    OIN\n",
       "0     1  1.750   0   t   f  uuuu    t    n  17.92\n",
       "1     0  0.290   1   f   f  wwww    f    n  16.92\n",
       "2     1  0.000   2   f   f  wwww    t    n  31.25\n",
       "3     0  0.335   3   f   f  uuuu    f    n  48.17\n",
       "4     0  0.500   4   t   f  wwww    f    n  32.33"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95249249-d570-4f8a-951e-5212b8fe6d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4070, 11)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('./data/Task1_1.csv', sep=';')\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d00980b4-1498-453f-8300-88dca59affc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UKL</th>\n",
       "      <th>GJAH</th>\n",
       "      <th>ZIK</th>\n",
       "      <th>HUI</th>\n",
       "      <th>ERZ</th>\n",
       "      <th>CDx</th>\n",
       "      <th>BJZHD</th>\n",
       "      <th>NKJUD</th>\n",
       "      <th>LPI</th>\n",
       "      <th>BJKG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>oooo</td>\n",
       "      <td>x</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>5.0</td>\n",
       "      <td>vvvv</td>\n",
       "      <td>80.0</td>\n",
       "      <td>800000.0</td>\n",
       "      <td>qqqq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>rrr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uuu</td>\n",
       "      <td>pppp</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mmm</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>qqqq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>oooo</td>\n",
       "      <td>x</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>19.0</td>\n",
       "      <td>hh</td>\n",
       "      <td>96.0</td>\n",
       "      <td>960000.0</td>\n",
       "      <td>hh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>oooo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>120.0</td>\n",
       "      <td>kkk</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>qqq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>oooo</td>\n",
       "      <td>y</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mmm</td>\n",
       "      <td>232.0</td>\n",
       "      <td>2320000.0</td>\n",
       "      <td>qqqq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  UKL  GJAH  ZIK   HUI   ERZ    CDx BJZHD  NKJUD        LPI  BJKG\n",
       "0   0  160  oooo    x  oooo   www    5.0  vvvv   80.0   800000.0  qqqq\n",
       "1   1  153   rrr  NaN   uuu  pppp    0.0   mmm  200.0  2000000.0  qqqq\n",
       "2   2    5  oooo    x  oooo   www   19.0    hh   96.0   960000.0    hh\n",
       "3   3    9  oooo  NaN  oooo   www  120.0   kkk    0.0        0.0   qqq\n",
       "4   4   40  oooo    y  oooo   www    0.0   mmm  232.0  2320000.0  qqqq"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6846134-2e28-4757-a7fa-c6c57dc1a746",
   "metadata": {},
   "source": [
    "## Preprocess & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd19ae1-5f6c-4872-9c5d-a1eee46d5b25",
   "metadata": {},
   "source": [
    "### Drop duplicates\n",
    "\n",
    "Prior to merging `df1` and `df2` we need to ensure unique records. We see below there are 370 duplicate rows, where \"duplicate\" is defined as having the same value for all columns in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "143312dc-b37f-49d6-b09d-2c126e408cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in df1: 370\n",
      "Number of duplicates in df2: 370\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of duplicates in df1: {df1.duplicated().sum()}\")\n",
    "print(f\"Number of duplicates in df2: {df2.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f9ae46-bf52-4829-9fa5-6f2df32fc830",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3700, 11)\n",
      "(3700, 9)\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.drop_duplicates().reset_index(drop=True)\n",
    "df2 = df2.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "assert df1.duplicated().sum() == 0  # sanity check\n",
    "assert df2.duplicated().sum() == 0\n",
    "\n",
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d7c6e-73d8-4c18-bbbe-ae068a643163",
   "metadata": {},
   "source": [
    "### Merge\n",
    "\n",
    "Although it's not explicitly stated in the instructions, I assume an inner join is desired. As we can see, there's a 100% match rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3bcc357-83a5-4641-a958-98c5c12a89a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3700, 19)\n"
     ]
    }
   ],
   "source": [
    "df = df1.merge(df2, on='ID', how='inner')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1279a10-d29d-4b45-82ce-c7a72123a8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UKL</th>\n",
       "      <th>GJAH</th>\n",
       "      <th>ZIK</th>\n",
       "      <th>HUI</th>\n",
       "      <th>ERZ</th>\n",
       "      <th>CDx</th>\n",
       "      <th>BJZHD</th>\n",
       "      <th>NKJUD</th>\n",
       "      <th>LPI</th>\n",
       "      <th>BJKG</th>\n",
       "      <th>POUG</th>\n",
       "      <th>TRE</th>\n",
       "      <th>ZUB</th>\n",
       "      <th>VOL</th>\n",
       "      <th>UIO</th>\n",
       "      <th>VBNM</th>\n",
       "      <th>Type</th>\n",
       "      <th>OIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>oooo</td>\n",
       "      <td>x</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>5.0</td>\n",
       "      <td>vvvv</td>\n",
       "      <td>80.0</td>\n",
       "      <td>800000.0</td>\n",
       "      <td>qqqq</td>\n",
       "      <td>1</td>\n",
       "      <td>1.750</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>uuuu</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>17.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>rrr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uuu</td>\n",
       "      <td>pppp</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mmm</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>qqqq</td>\n",
       "      <td>0</td>\n",
       "      <td>0.290</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>wwww</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>16.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>oooo</td>\n",
       "      <td>x</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>19.0</td>\n",
       "      <td>hh</td>\n",
       "      <td>96.0</td>\n",
       "      <td>960000.0</td>\n",
       "      <td>hh</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>wwww</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>31.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>oooo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>120.0</td>\n",
       "      <td>kkk</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>qqq</td>\n",
       "      <td>0</td>\n",
       "      <td>0.335</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>uuuu</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>48.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>oooo</td>\n",
       "      <td>y</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mmm</td>\n",
       "      <td>232.0</td>\n",
       "      <td>2320000.0</td>\n",
       "      <td>qqqq</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>wwww</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>32.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  UKL  GJAH  ZIK   HUI   ERZ    CDx BJZHD  NKJUD        LPI  BJKG  POUG  \\\n",
       "0   0  160  oooo    x  oooo   www    5.0  vvvv   80.0   800000.0  qqqq     1   \n",
       "1   1  153   rrr  NaN   uuu  pppp    0.0   mmm  200.0  2000000.0  qqqq     0   \n",
       "2   2    5  oooo    x  oooo   www   19.0    hh   96.0   960000.0    hh     1   \n",
       "3   3    9  oooo  NaN  oooo   www  120.0   kkk    0.0        0.0   qqq     0   \n",
       "4   4   40  oooo    y  oooo   www    0.0   mmm  232.0  2320000.0  qqqq     0   \n",
       "\n",
       "     TRE ZUB VOL   UIO VBNM Type    OIN  \n",
       "0  1.750   t   f  uuuu    t    n  17.92  \n",
       "1  0.290   f   f  wwww    f    n  16.92  \n",
       "2  0.000   f   f  wwww    t    n  31.25  \n",
       "3  0.335   f   f  uuuu    f    n  48.17  \n",
       "4  0.500   t   f  wwww    f    n  32.33  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c08b77a-e69d-43a2-aa93-c6f8b59197ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID column since no longer needed for modeling\n",
    "del df['ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c79c91-f228-4051-a20d-c3f2b77a60d9",
   "metadata": {},
   "source": [
    "### Class imbalance\n",
    "\n",
    "As the instructions suggested - and the data below confirms - this is a binary classification problem. Importantly, the target variable is highly class imbalanced (i.e. the distribution of classes is highly unequal), with ~92.5% of cases being \"y\", while only ~7.5% being \"n\". While phenomenon is common in real-world applications, it also poses some modeling challenges.\n",
    "\n",
    "To address this, I focus on performance metrics that differentiate between performance by class (e.g. precision, recall, F1 score, confusion matrix), rather than \"global\" performance indicators like accuracy. The reason being that in expectation, a model could be (in this case) about 92.5% accurate simply by predicting the dominant class for every observation, which would be a poor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be08d455-c5ea-47b4-9c15-8033af0728b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y    0.925405\n",
       "n    0.074595\n",
       "Name: Type, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1cca6-9599-4b7e-bc91-3f8e49810493",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "\n",
    "As we see below, several of our predictor features contain missing data (though not our target feature). Because I use an ensemble method below that averages predictions across three different ML models, two of which cannot easily handle missing data (SVM & random forest) I impute these missing values.\n",
    "\n",
    "Several imputation strategies exist for missing data, including:\n",
    "- listwise deletion - drop rows with any missing column values\n",
    "- unconditional imputation - use measure of central tendency (mean or median) among non-missing rows\n",
    "- conditional imputation - use e.g. a ML model to first impute the data iteratively by feature, using all other features\n",
    "\n",
    "While conditional imputation offers the best performance in expectation, for sake of time I use an unconditional imputation method.\n",
    "\n",
    "**For our modeling purposes, imputation is importantly only required for numeric features**, not categorical features. This is because we'll one-hot encode categorical features, which enables us to code \"missing\" as simply another feature value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83441ad0-266d-4925-89d9-7c6654da920d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UKL      0.000000\n",
       "GJAH     0.017297\n",
       "ZIK      0.579730\n",
       "HUI      0.000000\n",
       "ERZ      0.017297\n",
       "CDx      0.000000\n",
       "BJZHD    0.017838\n",
       "NKJUD    0.027027\n",
       "LPI      0.027027\n",
       "BJKG     0.017838\n",
       "POUG     0.000000\n",
       "TRE      0.000000\n",
       "ZUB      0.000000\n",
       "VOL      0.000000\n",
       "UIO      0.010541\n",
       "VBNM     0.000000\n",
       "Type     0.000000\n",
       "OIN      0.010541\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Share of missing data by column\n",
    "df.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c354917-2eed-4a7a-a346-1257e3281145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "numeric_cols.remove('POUG')  # this appears categorical so we remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5882f8a0-172f-4eab-90ab-d141db218e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UKL</th>\n",
       "      <th>CDx</th>\n",
       "      <th>NKJUD</th>\n",
       "      <th>LPI</th>\n",
       "      <th>TRE</th>\n",
       "      <th>OIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3700.000000</td>\n",
       "      <td>3700.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3.600000e+03</td>\n",
       "      <td>3700.000000</td>\n",
       "      <td>3661.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>95.688378</td>\n",
       "      <td>2246.705946</td>\n",
       "      <td>162.695000</td>\n",
       "      <td>1.626950e+06</td>\n",
       "      <td>3.439496</td>\n",
       "      <td>32.820713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>56.382436</td>\n",
       "      <td>8708.571126</td>\n",
       "      <td>156.045682</td>\n",
       "      <td>1.560457e+06</td>\n",
       "      <td>4.335229</td>\n",
       "      <td>12.666181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>28.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>152.000000</td>\n",
       "      <td>1059.750000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>2.800000e+06</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>40.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>179.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1160.000000</td>\n",
       "      <td>1.160000e+07</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>80.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               UKL            CDx        NKJUD           LPI          TRE  \\\n",
       "count  3700.000000    3700.000000  3600.000000  3.600000e+03  3700.000000   \n",
       "mean     95.688378    2246.705946   162.695000  1.626950e+06     3.439496   \n",
       "std      56.382436    8708.571126   156.045682  1.560457e+06     4.335229   \n",
       "min       1.000000       0.000000     0.000000  0.000000e+00     0.000000   \n",
       "25%      46.000000       0.000000     0.000000  0.000000e+00     0.500000   \n",
       "50%      99.000000     113.000000   120.000000  1.200000e+06     1.750000   \n",
       "75%     152.000000    1059.750000   280.000000  2.800000e+06     5.000000   \n",
       "max     179.000000  100000.000000  1160.000000  1.160000e+07    28.500000   \n",
       "\n",
       "               OIN  \n",
       "count  3661.000000  \n",
       "mean     32.820713  \n",
       "std      12.666181  \n",
       "min      13.750000  \n",
       "25%      23.000000  \n",
       "50%      28.670000  \n",
       "75%      40.830000  \n",
       "max      80.250000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistical moments before imputation\n",
    "df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "795cafa1-c660-440e-be92-838d8ca54def",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numeric_cols:\n",
    "    mu = df[df[col].notnull()][col].mean()  # mean as measure of central tendency\n",
    "    df[col] = np.where(df[col].isnull(), mu, df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8931f5b2-b3f0-4471-acf6-1cb2fcd27a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df[numeric_cols].isnull().mean().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5874239-324f-4860-82fd-aa961e15e6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UKL</th>\n",
       "      <th>CDx</th>\n",
       "      <th>NKJUD</th>\n",
       "      <th>LPI</th>\n",
       "      <th>TRE</th>\n",
       "      <th>OIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3700.000000</td>\n",
       "      <td>3700.000000</td>\n",
       "      <td>3700.000000</td>\n",
       "      <td>3.700000e+03</td>\n",
       "      <td>3700.000000</td>\n",
       "      <td>3700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>95.688378</td>\n",
       "      <td>2246.705946</td>\n",
       "      <td>162.695000</td>\n",
       "      <td>1.626950e+06</td>\n",
       "      <td>3.439496</td>\n",
       "      <td>32.820713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>56.382436</td>\n",
       "      <td>8708.571126</td>\n",
       "      <td>153.921934</td>\n",
       "      <td>1.539219e+06</td>\n",
       "      <td>4.335229</td>\n",
       "      <td>12.599232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>28.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>152.000000</td>\n",
       "      <td>1059.750000</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>2.740000e+06</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>179.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1160.000000</td>\n",
       "      <td>1.160000e+07</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>80.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               UKL            CDx        NKJUD           LPI          TRE  \\\n",
       "count  3700.000000    3700.000000  3700.000000  3.700000e+03  3700.000000   \n",
       "mean     95.688378    2246.705946   162.695000  1.626950e+06     3.439496   \n",
       "std      56.382436    8708.571126   153.921934  1.539219e+06     4.335229   \n",
       "min       1.000000       0.000000     0.000000  0.000000e+00     0.000000   \n",
       "25%      46.000000       0.000000     0.000000  0.000000e+00     0.500000   \n",
       "50%      99.000000     113.000000   120.000000  1.200000e+06     1.750000   \n",
       "75%     152.000000    1059.750000   274.000000  2.740000e+06     5.000000   \n",
       "max     179.000000  100000.000000  1160.000000  1.160000e+07    28.500000   \n",
       "\n",
       "               OIN  \n",
       "count  3700.000000  \n",
       "mean     32.820713  \n",
       "std      12.599232  \n",
       "min      13.750000  \n",
       "25%      23.000000  \n",
       "50%      28.670000  \n",
       "75%      40.000000  \n",
       "max      80.250000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify statistical moments appear similar after imputation\n",
    "df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de05ad-f6c0-4eb8-b39b-b5e68ec94a19",
   "metadata": {},
   "source": [
    "### One-hot encode categorical features\n",
    "\n",
    "One hot encoding a feature involves creating a new boolean feature to typically represent each unique value in source feature. To do this we use sklearn's `OneHotEncoder` class. Depending on the model (e.g. linear regression or regression using maximum likelihood without regularization), it's necessary to drop one category value, otherwise the one-hot columns will be perfectly collinear. However, collinearity is typically not a problem for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ea71e39-b949-437f-b485-1e01e8e967e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [i for i in df.columns if i not in numeric_cols and \"Type\" not in i]  # exclude target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e682026e-eb33-4a5d-8490-2bdd93803518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GJAH      4\n",
       "ZIK       3\n",
       "HUI       3\n",
       "ERZ       4\n",
       "BJZHD    13\n",
       "BJKG      9\n",
       "POUG     23\n",
       "ZUB       2\n",
       "VOL       2\n",
       "UIO       3\n",
       "VBNM      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique values among categorical features\n",
    "df[categorical_cols].nunique(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "156b64a5-03e3-4c58-9e9e-444654059385",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = OneHotEncoder().fit_transform(df[categorical_cols]).toarray()\n",
    "assert (df[categorical_cols].nunique(dropna=False).sum() == X.shape[1])  # ensure yields full number of unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd714bd1-7fcc-4e3d-8975-79e99784a2a6",
   "metadata": {},
   "source": [
    "#### Concatenate numeric features \n",
    "\n",
    "To complete our `X` matrix of predictor features, we concatenate the numeric features to our newly created array of one-hot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99a37b51-e603-400f-b738-e3617b308250",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X, df[numeric_cols].values), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f0d4a-a7f9-4ed0-9707-45909a5fdd87",
   "metadata": {},
   "source": [
    "### Label encode target feature\n",
    "\n",
    "Label encoding is the process of converting the non-numeric values of a categorical feature to a numeric representation. \n",
    "\n",
    "**Given the class imbalance described above, I recode the rarer \"n\" category to 1, and the more common \"y\" category to 0.** This coding procedure is common when modeling using imbalanced data, allow us to focus on classical measures of precision and recall, which assume the user is interested in the harder-to-predict class labeled 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0bb80a2-3cca-423b-b51c-33f1e155fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Type'].replace({'n': 1, 'y': 0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e93e94-cb2a-4d93-88e4-90ec27dc48b7",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "I split the data into train and test/holdout subsets, allowing me to evaluate the trained model performance on the unseen test set. Although this split is admittedly somewhat arbitrary since we have no \"true\" test set in this case, it nonetheless provides a straightforward way to evaluate model performance and compare across models on unseen data. \n",
    "\n",
    "*Note* - I use k-fold cross-validation below, so I do not create a traditional cross-validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acc41bc3-95f9-47fb-b4cd-bca9309912bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3330, 74)\n",
      "(370, 74)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4228c039-3c65-4834-a52f-47da1a240b02",
   "metadata": {},
   "source": [
    "### Data normalization\n",
    "\n",
    "We use sklearn's `MinMaxScaler`, which transforms each of our predictor features to 0-1 scale. Data normalization is used to convert the range of features to a similar scale, which speeds up training. An alternative method would be standardization (i.e. z-score normalization; mean of 0, std of 1).\n",
    "\n",
    "**Importantly normalization takes place *after* splitting the data into train and test sets**. This ensures not only that there is no data leakage between subsets, but also that the transformed distributions are consistent between subsets (e.g. all range from 0-1, since they're using the minimum and maximum values of that subset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c0ef353-fa14-44ac-93a3-b59214332f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_ = scaler.fit_transform(X_train)\n",
    "X_test_ = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fadba00-e31b-4281-a024-4c84e6b0e5f2",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e65f1a-0f3a-4616-a0b9-b4e23c73156d",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a486a21-c28d-4512-b0a8-53504660d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_true, y_pred):\n",
    "    '''\n",
    "    Calculates binary classification performance metrics for a given model.\n",
    "    :param y_true: array_like, truth values as int\n",
    "    :param y_pred: array_like, predicted values as int\n",
    "    :returns: dict, with keys for each metric: \n",
    "        accuracy - proportion of correct predictions out of total predictions\n",
    "        sensitivity - (aka recall), of all true positives how many did we correctly predict as positive\n",
    "        specificity - (aka selectivity/TNR), of all true negatives how many did we correctly predict as negative\n",
    "        precision - of all predicted positive cases how many were actually positive\n",
    "        F-1 score - harmonic/weighted mean of precision and sensitivity scores\n",
    "        ROC-AUC - area under receiver operating characteristic curve\n",
    "        \n",
    "    '''\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    metrics_dict = {}\n",
    "    rounding = 6\n",
    "    metrics_dict['accuracy'] = round((tp + tn) / len(y_true), rounding)\n",
    "    metrics_dict['sensitivity'] = round(tp / (fn + tp), rounding) # aka recall\n",
    "    metrics_dict['specificity'] = round(tn / (tn + fp), rounding) # aka TNR\n",
    "    metrics_dict['precision'] = round(tp / (tp + fp), rounding)\n",
    "    metrics_dict['f1'] = round(2 * (metrics_dict['precision'] * metrics_dict['sensitivity']) \\\n",
    "                        / (metrics_dict['precision'] + metrics_dict['sensitivity']), rounding)\n",
    "    metrics_dict['roc_auc'] = round(roc_auc_score(y_true, y_pred), rounding)\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe779be-e7c0-4ffd-a282-dc79aaf4162e",
   "metadata": {},
   "source": [
    "#### Support vector machines (SVM) \n",
    "\n",
    "SVM is a type of supervised ML algorithm use for classification and regression. In support vector classification, the model finds a hyperplane in n-dimensional space that maximizes the distance (margin) between classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a6e5009-9f4f-4a93-84f3-82b384e8f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(probability=True)\n",
    "svc.fit(X_train_, y_train)\n",
    "pred = svc.predict(X_test_)\n",
    "probab = svc.predict_proba(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c8db9af-7b6b-4a76-9962-0595000da21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.986486,\n",
       " 'sensitivity': 0.782609,\n",
       " 'specificity': 1.0,\n",
       " 'precision': 1.0,\n",
       " 'f1': 0.878049,\n",
       " 'roc_auc': 0.891304}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687263c-24f0-4028-806f-9f61d75d40bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
