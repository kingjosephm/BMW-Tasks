{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116d6993-240d-4d8a-88ca-eadeb19bad6a",
   "metadata": {},
   "source": [
    "# ***TASK 1***\n",
    "\n",
    "Description:\n",
    "\n",
    "\"This task is aiming to provide a binary classification of the column 'Type'. The dataset is provided into two different tables with unique identifier of column 'ID'. Tip: This column (ID) can be used to match the two tables.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25cd1d9a-4b66-4ac0-9203-bfc1d3f41a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import ComplementNB, CategoricalNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375fddab-bcac-4b33-8820-9066af26446a",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0765578-f155-43c8-a1d3-a87adab4974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4070, 9)\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('./data/Task1_2.csv', sep=';')\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89bde1ac-9b2f-4b3b-8040-0e39fede4fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POUG</th>\n",
       "      <th>TRE</th>\n",
       "      <th>ID</th>\n",
       "      <th>ZUB</th>\n",
       "      <th>VOL</th>\n",
       "      <th>UIO</th>\n",
       "      <th>VBNM</th>\n",
       "      <th>Type</th>\n",
       "      <th>OIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.750</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>uuuu</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>17.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.290</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>wwww</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>16.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>wwww</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>31.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.335</td>\n",
       "      <td>3</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>uuuu</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>48.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>4</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>wwww</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>32.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   POUG    TRE  ID ZUB VOL   UIO VBNM Type    OIN\n",
       "0     1  1.750   0   t   f  uuuu    t    n  17.92\n",
       "1     0  0.290   1   f   f  wwww    f    n  16.92\n",
       "2     1  0.000   2   f   f  wwww    t    n  31.25\n",
       "3     0  0.335   3   f   f  uuuu    f    n  48.17\n",
       "4     0  0.500   4   t   f  wwww    f    n  32.33"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95249249-d570-4f8a-951e-5212b8fe6d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4070, 11)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('./data/Task1_1.csv', sep=';')\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d00980b4-1498-453f-8300-88dca59affc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UKL</th>\n",
       "      <th>GJAH</th>\n",
       "      <th>ZIK</th>\n",
       "      <th>HUI</th>\n",
       "      <th>ERZ</th>\n",
       "      <th>CDx</th>\n",
       "      <th>BJZHD</th>\n",
       "      <th>NKJUD</th>\n",
       "      <th>LPI</th>\n",
       "      <th>BJKG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>oooo</td>\n",
       "      <td>x</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>5.0</td>\n",
       "      <td>vvvv</td>\n",
       "      <td>80.0</td>\n",
       "      <td>800000.0</td>\n",
       "      <td>qqqq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>rrr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uuu</td>\n",
       "      <td>pppp</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mmm</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>qqqq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>oooo</td>\n",
       "      <td>x</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>19.0</td>\n",
       "      <td>hh</td>\n",
       "      <td>96.0</td>\n",
       "      <td>960000.0</td>\n",
       "      <td>hh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>oooo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>120.0</td>\n",
       "      <td>kkk</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>qqq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>oooo</td>\n",
       "      <td>y</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mmm</td>\n",
       "      <td>232.0</td>\n",
       "      <td>2320000.0</td>\n",
       "      <td>qqqq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  UKL  GJAH  ZIK   HUI   ERZ    CDx BJZHD  NKJUD        LPI  BJKG\n",
       "0   0  160  oooo    x  oooo   www    5.0  vvvv   80.0   800000.0  qqqq\n",
       "1   1  153   rrr  NaN   uuu  pppp    0.0   mmm  200.0  2000000.0  qqqq\n",
       "2   2    5  oooo    x  oooo   www   19.0    hh   96.0   960000.0    hh\n",
       "3   3    9  oooo  NaN  oooo   www  120.0   kkk    0.0        0.0   qqq\n",
       "4   4   40  oooo    y  oooo   www    0.0   mmm  232.0  2320000.0  qqqq"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6846134-2e28-4757-a7fa-c6c57dc1a746",
   "metadata": {},
   "source": [
    "## Preprocess & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd19ae1-5f6c-4872-9c5d-a1eee46d5b25",
   "metadata": {},
   "source": [
    "### Drop duplicates\n",
    "\n",
    "Prior to merging `df1` and `df2` we need to ensure unique records. We see below there are 370 duplicate rows, where \"duplicate\" is defined as having the same value for all columns in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "143312dc-b37f-49d6-b09d-2c126e408cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in df1: 370\n",
      "Number of duplicates in df2: 370\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of duplicates in df1: {df1.duplicated().sum()}\")\n",
    "print(f\"Number of duplicates in df2: {df2.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f9ae46-bf52-4829-9fa5-6f2df32fc830",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3700, 11)\n",
      "(3700, 9)\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.drop_duplicates().reset_index(drop=True)\n",
    "df2 = df2.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "assert df1.duplicated().sum() == 0  # sanity check\n",
    "assert df2.duplicated().sum() == 0\n",
    "\n",
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d7c6e-73d8-4c18-bbbe-ae068a643163",
   "metadata": {},
   "source": [
    "### Merge\n",
    "\n",
    "Although it's not explicitly stated in the instructions, I assume an inner join is desired. As we can see, there's a 100% match rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3bcc357-83a5-4641-a958-98c5c12a89a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3700, 19)\n"
     ]
    }
   ],
   "source": [
    "df = df1.merge(df2, on='ID', how='inner')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1279a10-d29d-4b45-82ce-c7a72123a8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UKL</th>\n",
       "      <th>GJAH</th>\n",
       "      <th>ZIK</th>\n",
       "      <th>HUI</th>\n",
       "      <th>ERZ</th>\n",
       "      <th>CDx</th>\n",
       "      <th>BJZHD</th>\n",
       "      <th>NKJUD</th>\n",
       "      <th>LPI</th>\n",
       "      <th>BJKG</th>\n",
       "      <th>POUG</th>\n",
       "      <th>TRE</th>\n",
       "      <th>ZUB</th>\n",
       "      <th>VOL</th>\n",
       "      <th>UIO</th>\n",
       "      <th>VBNM</th>\n",
       "      <th>Type</th>\n",
       "      <th>OIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>oooo</td>\n",
       "      <td>x</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>5.0</td>\n",
       "      <td>vvvv</td>\n",
       "      <td>80.0</td>\n",
       "      <td>800000.0</td>\n",
       "      <td>qqqq</td>\n",
       "      <td>1</td>\n",
       "      <td>1.750</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>uuuu</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>17.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>rrr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uuu</td>\n",
       "      <td>pppp</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mmm</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>qqqq</td>\n",
       "      <td>0</td>\n",
       "      <td>0.290</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>wwww</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>16.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>oooo</td>\n",
       "      <td>x</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>19.0</td>\n",
       "      <td>hh</td>\n",
       "      <td>96.0</td>\n",
       "      <td>960000.0</td>\n",
       "      <td>hh</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>wwww</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>31.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>oooo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>120.0</td>\n",
       "      <td>kkk</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>qqq</td>\n",
       "      <td>0</td>\n",
       "      <td>0.335</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>uuuu</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>48.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>oooo</td>\n",
       "      <td>y</td>\n",
       "      <td>oooo</td>\n",
       "      <td>www</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mmm</td>\n",
       "      <td>232.0</td>\n",
       "      <td>2320000.0</td>\n",
       "      <td>qqqq</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>wwww</td>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>32.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  UKL  GJAH  ZIK   HUI   ERZ    CDx BJZHD  NKJUD        LPI  BJKG  POUG  \\\n",
       "0   0  160  oooo    x  oooo   www    5.0  vvvv   80.0   800000.0  qqqq     1   \n",
       "1   1  153   rrr  NaN   uuu  pppp    0.0   mmm  200.0  2000000.0  qqqq     0   \n",
       "2   2    5  oooo    x  oooo   www   19.0    hh   96.0   960000.0    hh     1   \n",
       "3   3    9  oooo  NaN  oooo   www  120.0   kkk    0.0        0.0   qqq     0   \n",
       "4   4   40  oooo    y  oooo   www    0.0   mmm  232.0  2320000.0  qqqq     0   \n",
       "\n",
       "     TRE ZUB VOL   UIO VBNM Type    OIN  \n",
       "0  1.750   t   f  uuuu    t    n  17.92  \n",
       "1  0.290   f   f  wwww    f    n  16.92  \n",
       "2  0.000   f   f  wwww    t    n  31.25  \n",
       "3  0.335   f   f  uuuu    f    n  48.17  \n",
       "4  0.500   t   f  wwww    f    n  32.33  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c08b77a-e69d-43a2-aa93-c6f8b59197ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID column since no longer needed for modeling\n",
    "del df['ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c79c91-f228-4051-a20d-c3f2b77a60d9",
   "metadata": {},
   "source": [
    "### Class imbalance\n",
    "\n",
    "As the instructions suggested - and the data below confirms - this is a binary classification problem. Importantly, the target variable is highly class imbalanced (i.e. the distribution of classes is highly unequal), with ~92.5% of cases being \"y\", while only ~7.5% being \"n\". While phenomenon is common in real-world applications, it also poses some modeling challenges.\n",
    "\n",
    "To address this, **I focus on performance metrics that differentiate between performance by class (e.g. precision, recall, F1 score, balanced accuracy), rather than \"global\" performance indicators like accuracy. The reason being that in expectation, a model could be (in this case) about 92.5% accurate simply by predicting the dominant class for every observation, which would be a poor model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be08d455-c5ea-47b4-9c15-8033af0728b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y    0.925405\n",
       "n    0.074595\n",
       "Name: Type, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1cca6-9599-4b7e-bc91-3f8e49810493",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "\n",
    "As we see below, several of our predictor features contain missing data (though not our target feature). Because I use an ensemble method below that averages predictions across three different ML models, two of which cannot easily handle missing data (SVM & gradient boosted trees) I impute these missing values.\n",
    "\n",
    "Several imputation strategies exist for missing data, including:\n",
    "- listwise deletion - drop rows with any missing column values\n",
    "- unconditional imputation - use measure of central tendency (mean or median) among non-missing rows\n",
    "- conditional imputation - use e.g. a ML model to first impute the data iteratively by feature, using all other features\n",
    "\n",
    "While conditional imputation offers the best performance in expectation, for sake of time I use an unconditional imputation method.\n",
    "\n",
    "**For our modeling purposes, imputation is importantly only required for numeric features**, not categorical features. This is because we'll one-hot encode categorical features, which enables us to code \"missing\" as simply another feature value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83441ad0-266d-4925-89d9-7c6654da920d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UKL      0.000000\n",
       "GJAH     0.017297\n",
       "ZIK      0.579730\n",
       "HUI      0.000000\n",
       "ERZ      0.017297\n",
       "CDx      0.000000\n",
       "BJZHD    0.017838\n",
       "NKJUD    0.027027\n",
       "LPI      0.027027\n",
       "BJKG     0.017838\n",
       "POUG     0.000000\n",
       "TRE      0.000000\n",
       "ZUB      0.000000\n",
       "VOL      0.000000\n",
       "UIO      0.010541\n",
       "VBNM     0.000000\n",
       "Type     0.000000\n",
       "OIN      0.010541\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Share of missing data by column\n",
    "df.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c354917-2eed-4a7a-a346-1257e3281145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "numeric_cols.remove('POUG')  # this appears categorical so we remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5882f8a0-172f-4eab-90ab-d141db218e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UKL</th>\n",
       "      <th>CDx</th>\n",
       "      <th>NKJUD</th>\n",
       "      <th>LPI</th>\n",
       "      <th>TRE</th>\n",
       "      <th>OIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3700.000000</td>\n",
       "      <td>3700.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3.600000e+03</td>\n",
       "      <td>3700.000000</td>\n",
       "      <td>3661.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>95.688378</td>\n",
       "      <td>2246.705946</td>\n",
       "      <td>162.695000</td>\n",
       "      <td>1.626950e+06</td>\n",
       "      <td>3.439496</td>\n",
       "      <td>32.820713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>56.382436</td>\n",
       "      <td>8708.571126</td>\n",
       "      <td>156.045682</td>\n",
       "      <td>1.560457e+06</td>\n",
       "      <td>4.335229</td>\n",
       "      <td>12.666181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>28.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>152.000000</td>\n",
       "      <td>1059.750000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>2.800000e+06</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>40.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>179.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1160.000000</td>\n",
       "      <td>1.160000e+07</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>80.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               UKL            CDx        NKJUD           LPI          TRE  \\\n",
       "count  3700.000000    3700.000000  3600.000000  3.600000e+03  3700.000000   \n",
       "mean     95.688378    2246.705946   162.695000  1.626950e+06     3.439496   \n",
       "std      56.382436    8708.571126   156.045682  1.560457e+06     4.335229   \n",
       "min       1.000000       0.000000     0.000000  0.000000e+00     0.000000   \n",
       "25%      46.000000       0.000000     0.000000  0.000000e+00     0.500000   \n",
       "50%      99.000000     113.000000   120.000000  1.200000e+06     1.750000   \n",
       "75%     152.000000    1059.750000   280.000000  2.800000e+06     5.000000   \n",
       "max     179.000000  100000.000000  1160.000000  1.160000e+07    28.500000   \n",
       "\n",
       "               OIN  \n",
       "count  3661.000000  \n",
       "mean     32.820713  \n",
       "std      12.666181  \n",
       "min      13.750000  \n",
       "25%      23.000000  \n",
       "50%      28.670000  \n",
       "75%      40.830000  \n",
       "max      80.250000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistical moments before imputation\n",
    "df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "795cafa1-c660-440e-be92-838d8ca54def",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numeric_cols:\n",
    "    mu = df[df[col].notnull()][col].mean()  # mean as measure of central tendency\n",
    "    df[col] = np.where(df[col].isnull(), mu, df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8931f5b2-b3f0-4471-acf6-1cb2fcd27a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df[numeric_cols].isnull().mean().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5874239-324f-4860-82fd-aa961e15e6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UKL</th>\n",
       "      <th>CDx</th>\n",
       "      <th>NKJUD</th>\n",
       "      <th>LPI</th>\n",
       "      <th>TRE</th>\n",
       "      <th>OIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3700.000000</td>\n",
       "      <td>3700.000000</td>\n",
       "      <td>3700.000000</td>\n",
       "      <td>3.700000e+03</td>\n",
       "      <td>3700.000000</td>\n",
       "      <td>3700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>95.688378</td>\n",
       "      <td>2246.705946</td>\n",
       "      <td>162.695000</td>\n",
       "      <td>1.626950e+06</td>\n",
       "      <td>3.439496</td>\n",
       "      <td>32.820713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>56.382436</td>\n",
       "      <td>8708.571126</td>\n",
       "      <td>153.921934</td>\n",
       "      <td>1.539219e+06</td>\n",
       "      <td>4.335229</td>\n",
       "      <td>12.599232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>28.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>152.000000</td>\n",
       "      <td>1059.750000</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>2.740000e+06</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>179.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1160.000000</td>\n",
       "      <td>1.160000e+07</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>80.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               UKL            CDx        NKJUD           LPI          TRE  \\\n",
       "count  3700.000000    3700.000000  3700.000000  3.700000e+03  3700.000000   \n",
       "mean     95.688378    2246.705946   162.695000  1.626950e+06     3.439496   \n",
       "std      56.382436    8708.571126   153.921934  1.539219e+06     4.335229   \n",
       "min       1.000000       0.000000     0.000000  0.000000e+00     0.000000   \n",
       "25%      46.000000       0.000000     0.000000  0.000000e+00     0.500000   \n",
       "50%      99.000000     113.000000   120.000000  1.200000e+06     1.750000   \n",
       "75%     152.000000    1059.750000   274.000000  2.740000e+06     5.000000   \n",
       "max     179.000000  100000.000000  1160.000000  1.160000e+07    28.500000   \n",
       "\n",
       "               OIN  \n",
       "count  3700.000000  \n",
       "mean     32.820713  \n",
       "std      12.599232  \n",
       "min      13.750000  \n",
       "25%      23.000000  \n",
       "50%      28.670000  \n",
       "75%      40.000000  \n",
       "max      80.250000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify statistical moments appear similar after imputation\n",
    "df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de05ad-f6c0-4eb8-b39b-b5e68ec94a19",
   "metadata": {},
   "source": [
    "### One-hot encode categorical features\n",
    "\n",
    "One hot encoding a feature involves creating a new boolean feature to typically represent each unique value in source feature. To do this we use sklearn's `OneHotEncoder` class. Depending on the model (e.g. linear regression or regression using maximum likelihood without regularization), it's necessary to drop one category value, otherwise the one-hot columns will be perfectly collinear. However, collinearity is typically not a problem for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ea71e39-b949-437f-b485-1e01e8e967e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [i for i in df.columns if i not in numeric_cols and \"Type\" not in i]  # exclude target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e682026e-eb33-4a5d-8490-2bdd93803518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GJAH      4\n",
       "ZIK       3\n",
       "HUI       3\n",
       "ERZ       4\n",
       "BJZHD    13\n",
       "BJKG      9\n",
       "POUG     23\n",
       "ZUB       2\n",
       "VOL       2\n",
       "UIO       3\n",
       "VBNM      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique values among categorical features\n",
    "df[categorical_cols].nunique(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "156b64a5-03e3-4c58-9e9e-444654059385",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = OneHotEncoder().fit_transform(df[categorical_cols]).toarray()\n",
    "assert (df[categorical_cols].nunique(dropna=False).sum() == X.shape[1])  # ensure yields full number of unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd714bd1-7fcc-4e3d-8975-79e99784a2a6",
   "metadata": {},
   "source": [
    "#### Concatenate numeric features \n",
    "\n",
    "To complete our `X` matrix of predictor features, we concatenate the numeric features to our newly created array of one-hot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99a37b51-e603-400f-b738-e3617b308250",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X, df[numeric_cols].values), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f0d4a-a7f9-4ed0-9707-45909a5fdd87",
   "metadata": {},
   "source": [
    "### Label encode target feature\n",
    "\n",
    "Label encoding is the process of converting the non-numeric values of a categorical feature to a numeric representation. \n",
    "\n",
    "**Given the class imbalance described above, I recode the rarer \"n\" category to 1, and the more common \"y\" category to 0.** This coding procedure is common when modeling using imbalanced data, allow us to focus on classical measures of precision and recall, which assume the user is interested in the harder-to-predict class labeled 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0bb80a2-3cca-423b-b51c-33f1e155fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Type'].replace({'n': 1, 'y': 0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e93e94-cb2a-4d93-88e4-90ec27dc48b7",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "I split the data into train and test/holdout subsets, allowing me to evaluate the trained model performance on the unseen test set. Although this split is admittedly somewhat arbitrary since we have no \"true\" test set in this case, it nonetheless provides a straightforward way to evaluate model performance and compare across models on unseen data. \n",
    "\n",
    "**Note** - An alternative would have been to use the full dataset, combined with [k-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics). However, since I am already using cross-validation to search for optimal model hyperparameters (see below), this would've added additional complexity to this short assigment, so I stuck with simpler train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acc41bc3-95f9-47fb-b4cd-bca9309912bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3330, 74)\n",
      "(370, 74)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4228c039-3c65-4834-a52f-47da1a240b02",
   "metadata": {},
   "source": [
    "### Data normalization\n",
    "\n",
    "We use sklearn's `MinMaxScaler`, which transforms each of our predictor features to 0-1 scale. Data normalization is used to convert the range of features to a similar scale, which speeds up training. An alternative method would be standardization (i.e. z-score normalization; mean of 0, std of 1).\n",
    "\n",
    "**Importantly normalization takes place *after* splitting the data into train and test sets**. This ensures not only that there is no data leakage between subsets, but also that the transformed distributions are consistent between subsets (e.g. all range from 0-1, since they're using the minimum and maximum values of that subset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c0ef353-fa14-44ac-93a3-b59214332f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_ = scaler.fit_transform(X_train)\n",
    "X_test_ = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fadba00-e31b-4281-a024-4c84e6b0e5f2",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Grid search explanation...\n",
    "\n",
    "Auxilliary results showed performance metrics from all hyperoptimized models outperformed the default hyperarameters of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e65f1a-0f3a-4616-a0b9-b4e23c73156d",
   "metadata": {},
   "source": [
    "### Performance metrics\n",
    "\n",
    "The choice of which particular performance metric(s) to focus on depends in large part on the requirements of the modelling task. Without knowing more about the target feature, `Type`, or the severity of various prediction errors ([type I vs. type 2](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors)), I choose to focus on the `f1 score`, which is interpreted as the harmonic mean of prediction and recall, ranging between 0-1, with 1 being best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a486a21-c28d-4512-b0a8-53504660d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_true, y_pred):\n",
    "    '''\n",
    "    Calculates binary classification performance metrics for a given model.\n",
    "    :param y_true: array_like, truth values as int\n",
    "    :param y_pred: array_like, predicted values as int\n",
    "    :returns: dict, with keys for each metric: \n",
    "        accuracy - proportion of correct predictions out of total predictions\n",
    "        balanced accuracy - average of recall obtained on each class\n",
    "        sensitivity - (aka recall), of all true positives how many did we correctly predict as positive\n",
    "        specificity - (aka selectivity/TNR), of all true negatives how many did we correctly predict as negative\n",
    "        precision - of all predicted positive cases how many were actually positive\n",
    "        F1 score - harmonic/weighted mean of precision and sensitivity scores\n",
    "        ROC-AUC - area under receiver operating characteristic curve\n",
    "        \n",
    "    '''\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    metrics_dict = {}\n",
    "    rounding = 6\n",
    "    metrics_dict['accuracy'] = round((tp + tn) / len(y_true), rounding)\n",
    "    metrics_dict['bal_accuracy'] = round(balanced_accuracy_score(y_true, y_pred), rounding)\n",
    "    metrics_dict['sensitivity'] = round(tp / (fn + tp), rounding) # aka recall\n",
    "    metrics_dict['specificity'] = round(tn / (tn + fp), rounding) # aka TNR\n",
    "    metrics_dict['precision'] = round(tp / (tp + fp), rounding)\n",
    "    metrics_dict['f1'] = round(2 * (metrics_dict['precision'] * metrics_dict['sensitivity']) \\\n",
    "                        / (metrics_dict['precision'] + metrics_dict['sensitivity']), rounding)\n",
    "    metrics_dict['roc_auc'] = round(roc_auc_score(y_true, y_pred), rounding)\n",
    "    \n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd11b34-9caf-42aa-94dd-0267ddec7203",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe779be-e7c0-4ffd-a282-dc79aaf4162e",
   "metadata": {},
   "source": [
    "#### Support vector machines (SVM) \n",
    "\n",
    "SVM is a type of supervised ML algorithm use for classification and regression. In support vector classification, the model finds a hyperplane in n-dimensional space that maximizes the distance (margin) between classes. \n",
    "\n",
    "I optimize the model hyperaparameters `C` and `kernel`. Whereas the former controls the regularization parameter (squared l2 penalty), the latter specifies the kernel type to be used by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60c9faac-a60e-47ec-a26d-b13e988e23c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Best model hyperparameters: '{'C': 10, 'kernel': 'rbf'}'\n",
      "\n",
      "Performance metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.997297,\n",
       " 'bal_accuracy': 0.978261,\n",
       " 'sensitivity': 0.956522,\n",
       " 'specificity': 1.0,\n",
       " 'precision': 1.0,\n",
       " 'f1': 0.977778,\n",
       " 'roc_auc': 0.978261}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"C\": [0.1, 1.0, 10, 100],\n",
    "          \"kernel\": ['rbf', 'poly']}  # Hyperparameters to grid search\n",
    "\n",
    "svc = GridSearchCV(estimator = SVC(), \n",
    "                    param_grid = params,\n",
    "                    scoring='f1', \n",
    "                    n_jobs=-1, \n",
    "                    verbose=1)\n",
    "\n",
    "svc.fit(X_train_, y_train)\n",
    "\n",
    "print(f\"\\nBest model hyperparameters: '{svc.best_params_}'\")\n",
    "\n",
    "pred_svc = svc.predict(X_test_)\n",
    "\n",
    "print(\"\\nPerformance metrics:\")\n",
    "metrics(y_test, pred_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c507b-3551-4f93-8e21-e8c704926cf1",
   "metadata": {},
   "source": [
    "#### Naive Bayes: CategoricalNB\n",
    "\n",
    "Naive Bayes is a family of simple, supervised learning algorithms based on Bayes' theorem combined with a strong \"naive\" assumption of conditional independence between features given the value of the outcome variable. Although this assumption is frequently violated in many real-world applications, naive Bayes algorithms often work well as classifiers (e.g. especially in document classification and spam filtering). An explanation for this paradox can be found [here](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf); essentially, the answer lies in the distribution of dependence within a class often cancelling each other out. Importantly, naive Bayes models are poor estimators, meaning the probability outputs should not be relied upon. \n",
    "\n",
    "Comparing various naive Bayes implementations (GaussianNB, MultinomialNB, ComplementNB, and CategoricalNB), model performance was markedly better with Categorical Naive Bayes, according to auxilliary analyses. \n",
    "\n",
    "For this model, I optimize two hyperparameters, `alpha`, and `min_categories`, which are the additive (Laplace) smoother, and the minimum number of categories per features. Auxilliary analyses showed that this flavor of naive Bayes struggled when it encountered unseen values in the test set, which this hyperparameters resolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f12602b9-7f41-45b8-b0bb-e10b1da3c308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
      "\n",
      "Best model hyperparameters: '{'alpha': 0.1, 'min_categories': 100}'\n",
      "\n",
      "Performance metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.910811,\n",
       " 'bal_accuracy': 0.810362,\n",
       " 'sensitivity': 0.695652,\n",
       " 'specificity': 0.925072,\n",
       " 'precision': 0.380952,\n",
       " 'f1': 0.492307,\n",
       " 'roc_auc': 0.810362}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"alpha\": [0.1, 1.0, 10],\n",
    "          \"min_categories\": [2, 5, 10, 20, 50, 100, 500]}\n",
    "\n",
    "nb = GridSearchCV(estimator = CategoricalNB(force_alpha=True), \n",
    "                    param_grid = params,\n",
    "                    scoring='f1', \n",
    "                    n_jobs=-1, \n",
    "                    verbose=1)\n",
    "\n",
    "nb.fit(X_train_, y_train)\n",
    "\n",
    "print(f\"\\nBest model hyperparameters: '{nb.best_params_}'\")\n",
    "\n",
    "pred_nb = nb.predict(X_test_)\n",
    "\n",
    "print(\"\\nPerformance metrics:\")\n",
    "metrics(y_test, pred_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c35363b-bfe1-4681-89d3-20908c13640d",
   "metadata": {},
   "source": [
    "#### Boosted decision tree\n",
    "\n",
    "TODO\n",
    "\n",
    "As compared to the other two models, boosted decision trees have a considerable number of hyperparameters to optimize. I choose to focus on a key set of hyperparameters that tend to have the largest influence on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7f9554d-5020-4176-9989-d3ed851b61bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.981081,\n",
       " 'bal_accuracy': 0.929019,\n",
       " 'sensitivity': 0.869565,\n",
       " 'specificity': 0.988473,\n",
       " 'precision': 0.833333,\n",
       " 'f1': 0.851064,\n",
       " 'roc_auc': 0.929019}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train_, y_train)\n",
    "pred_gbc = gbc.predict(X_test_)\n",
    "metrics(y_test, pred_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f56fb76c-ca33-4069-aa4e-7b24bbc8bbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Best model hyperparameters: '{'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 1000}'\n",
      "\n",
      "Performance metrics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.986486,\n",
       " 'bal_accuracy': 0.972497,\n",
       " 'sensitivity': 0.956522,\n",
       " 'specificity': 0.988473,\n",
       " 'precision': 0.846154,\n",
       " 'f1': 0.897959,\n",
       " 'roc_auc': 0.972497}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "params = {\"learning_rate\": [0.05, 0.1, 0.2],\n",
    "          \"n_estimators\": [50, 100, 500, 1000],\n",
    "          \"max_depth\": [3, 5, 7, 9, 11]}\n",
    "\n",
    "gbc = GridSearchCV(estimator = GradientBoostingClassifier(), \n",
    "                    param_grid = params,\n",
    "                    scoring='f1', \n",
    "                    n_jobs=-1, \n",
    "                    verbose=1)\n",
    "\n",
    "gbc.fit(X_train_, y_train)\n",
    "\n",
    "print(f\"\\nBest model hyperparameters: '{gbc.best_params_}'\")\n",
    "\n",
    "pred_gbc = gbc.predict(X_test_)\n",
    "\n",
    "print(\"\\nPerformance metrics:\")\n",
    "metrics(y_test, pred_gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502feee5-a168-4929-981e-d02c1a88a7f7",
   "metadata": {},
   "source": [
    "### Ensembling estimators using hard voting\n",
    "\n",
    "Ensembling is a heterogenous method of combining..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33f060fc-54e3-4f8c-a5d3-bd74f3054cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = pd.DataFrame([pred_svc, pred_nb,  pred_gbc]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "118cb380-8597-4c2f-b92c-a246bc477159",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = ensemble.apply(lambda x: x.value_counts().index[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d5d57d4-447b-443c-9478-d49ba18451e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.991892,\n",
       " 'bal_accuracy': 0.975379,\n",
       " 'sensitivity': 0.956522,\n",
       " 'specificity': 0.994236,\n",
       " 'precision': 0.916667,\n",
       " 'f1': 0.936171,\n",
       " 'roc_auc': 0.975379}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics(y_test, ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e01d6-d2fb-4e62-9af5-c5b70cee8dec",
   "metadata": {},
   "source": [
    "##### Sns barplot of f1 comparing different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d57b1f-3a08-4c55-a822-ae149906916e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
